import tempfile
import torch
import os
import datajoint as dj
import numpy as np
from nnfabrik.templates.checkpoint import TrainedModelChkptBase
from nnfabrik.utility.dj_helpers import gitlog, make_hash
from nnfabrik.templates.trained_model import TrainedModelBase


class TransferredTrainedModelBase(TrainedModelBase):
    """
    A modified version of TrainedModel table which enables step-wise table population given the
    step specification. Refer to the corresponding example notebook for a demo.
    """

    table_comment = "Transferred trained models"

    @property
    def definition(self):
        definition = """
        # {table_comment}
        transfer_step:                     int          # transfer step
        -> self().model_table
        -> self().dataset_table
        -> self().trainer_table
        -> self().seed_table
        prev_model_fn:                     varchar(64)
        prev_model_hash:                   varchar(64)
        prev_dataset_fn:                   varchar(64)
        prev_dataset_hash:                 varchar(64)
        prev_trainer_fn:                   varchar(64)
        prev_trainer_hash:                 varchar(64)
        collapsed_history:                 varchar(64)
        data_transfer:                     tinyint 
        ---
        comment='':                        varchar(768) # short description 
        score:                             float        # loss
        output:                            longblob     # trainer object's output
        ->[nullable] self().user_table
        trainedmodel_ts=CURRENT_TIMESTAMP: timestamp    # UTZ timestamp at time of insertion
        current_model_fn:                  varchar(64)
        current_model_hash:                varchar(64)
        current_dataset_fn:                varchar(64)
        current_dataset_hash:              varchar(64)
        current_trainer_fn:                varchar(64)
        current_trainer_hash:              varchar(64)
        """.format(
            table_comment=self.table_comment
        )
        return definition

    class ModelStorage(TrainedModelBase.ModelStorage):
        pass

    class DataStorage(dj.Part):
        @property
        def definition(self):
            definition = """
            # Contains the data generated by the transfer step, stored externally.
            -> master
            ---
            transfer_data:            attach@{storage}
            """.format(
                storage=self._master.storage
            )
            return definition

    class CollapsedHistory(dj.Part):
        """
        For the result of two or more transfer steps to be uniquely identifiable,
        we compress its entire history (the keys of all previous steps) into a single hash (`collapsed_history`).
        This table keeps track of this process and can be used to recursively retrieve the transfer history.
        """

        definition = """
        collapsed_history:                 varchar(64)
        prev_model_fn:                     varchar(64)
        prev_model_hash:                   varchar(64)
        prev_dataset_fn:                   varchar(64)
        prev_dataset_hash:                 varchar(64)
        prev_trainer_fn:                   varchar(64)
        prev_trainer_hash:                 varchar(64)
        prev_collapsed_history:             varchar(64)
        """

        @classmethod
        def add_entry(cls, key):
            key = {
                "prev_model_fn": key["prev_model_fn"],
                "prev_model_hash": key["prev_model_hash"],
                "prev_dataset_fn": key["prev_dataset_fn"],
                "prev_dataset_hash": key["prev_dataset_hash"],
                "prev_trainer_fn": key["prev_trainer_fn"],
                "prev_trainer_hash": key["prev_trainer_hash"],
                "prev_collapsed_history": key["collapsed_history"],
            }
            key["collapsed_history"] = make_hash(key)
            cls.insert1(key, skip_duplicates=True)

    def _transfer_recipe(self, transfer_step):
        """
        Combines multiple transfer recipes and their resitrictions as specified by post_restr attribute.
        The combination is transfer-step-specific, meaning only the recipes relevant for a specific transfer step would be combined.

        Combining recipes are pretty easy and the user does not need to interact with this method directly. Below is an example:
        Let us assume you have two recipe tables: TrainerRecipe and ModelRecipe, the you can attach all these recipes to your
        TransferTrainedModel table as follow:

        ``` Python
            TransferTrainedModel.transfer_recipe = [TrainerRecipe, ModelRecipe]
        ```

        The rest (combining the recipes and their restrictions) is taken care of by this method.

        Args:
            transfer_step (int): table population trasnfer step.

        Returns:
            string or datajoint AndList: A single or combined restriction of one or multiple recipes, respectively.
        """

        if isinstance(self.transfer_recipe, list):
            # get the recipes that have an entry for a specific transfer step
            transfer_recipe = []
            # loop over the transfer recipes
            for tr in self.transfer_recipe:
                # check if an entry exists for a specific transfer step in the recipe
                if tr & f"transfer_step = {transfer_step}":
                    # if it exists add that entry to the list of recipes (relevant for a specific transfer step)
                    transfer_recipe.append(tr & f"transfer_step = {transfer_step}")
            if not transfer_recipe:
                return self.proj() - self  # return something empty
            # join all the recipes (and their post_restr)
            joined = transfer_recipe[0]
            if len(transfer_recipe) > 1:
                for t in transfer_recipe[1:]:
                    joined *= t  # all combination of recipes
                joined.post_restr = dj.AndList(
                    [recipe.post_restr for recipe in self.transfer_recipe]
                )
            return joined
        else:
            return self.transfer_recipe

    @property
    def key_source(self):
        key_source = None
        if hasattr(self, "transfer_recipe"):
            # map "prev_"-attributes and "collapsed_history" to their corresponding (updated) collapsed history
            with_collapsed_history = (
                self.proj(
                    "current_model_fn",
                    "current_model_hash",
                    "current_dataset_fn",
                    "current_dataset_hash",
                    "current_trainer_fn",
                    "current_trainer_hash",
                    prev_collapsed_history="collapsed_history",
                )
                * self.CollapsedHistory
            )
            # project (rename) attributes of the existing transferredmodel table to the same name but with prefix "prev"
            prev_transferred_model = with_collapsed_history.proj(
                prev_model_fn="current_model_fn",
                prev_model_hash="current_model_hash",
                prev_dataset_fn="current_dataset_fn",
                prev_dataset_hash="current_dataset_hash",
                prev_trainer_fn="current_trainer_fn",
                prev_trainer_hash="current_trainer_hash",
                prev_step="transfer_step",
                transfer_step="transfer_step + 1",
                collapsed_history="collapsed_history",
                _data_transfer="data_transfer",  # rename so this entry in the recipe is not used for restriction
            ) * dj.U(
                "transfer_step",  # make these attributes primary keys
                "prev_model_fn",
                "prev_model_hash",
                "prev_dataset_fn",
                "prev_dataset_hash",
                "prev_trainer_fn",
                "prev_trainer_hash",
                "collapsed_history",
            )

            # get the current transfer step
            max_transfer_step = (
                prev_transferred_model.fetch("transfer_step").max()
                if prev_transferred_model
                else 0
            )

            for transfer_step in range(1, max_transfer_step + 1):
                # get the necessay attributes to filter the prev_transferredmodel with the transfer recipe
                prev_transferred_model = (
                    dj.U(
                        "transfer_step",
                        "prev_model_fn",
                        "prev_model_hash",
                        "prev_dataset_fn",
                        "prev_dataset_hash",
                        "prev_trainer_fn",
                        "prev_trainer_hash",
                        "collapsed_history",
                        "_data_transfer",
                    )
                    & prev_transferred_model
                )

                # get the entries that match the one in TransferRecipe (all entries that have matching "prev_...")
                recipe = self._transfer_recipe(transfer_step)
                post_restr = recipe.post_restr if recipe else dj.AndList([])
                transfer_from = prev_transferred_model * recipe
                transfers = (
                    dj.U(
                        "transfer_step",
                        "model_fn",
                        "model_hash",
                        "dataset_fn",
                        "dataset_hash",
                        "trainer_fn",
                        "trainer_hash",
                        "seed",
                        "prev_model_fn",
                        "prev_model_hash",
                        "prev_dataset_fn",
                        "prev_dataset_hash",
                        "prev_trainer_fn",
                        "prev_trainer_hash",
                        "collapsed_history",
                        "data_transfer",
                    )
                    & (
                        self.model_table
                        * self.dataset_table
                        * self.trainer_table
                        * self.seed_table
                        * transfer_from
                    )  # combine recipe restriction with all possible training combinations
                    & post_restr  # restrict with post_rest
                )
                if key_source is not None:
                    key_source = key_source + transfers.proj()
                else:
                    key_source = transfers.proj()

        # normal entries as a combination of Dataset, Model, Trainer, and Seed tables
        step_0 = (
            self.model_table * self.dataset_table * self.trainer_table * self.seed_table
        )
        # add transfer_step and prev_hash as prim keys
        base = dj.U(
            "transfer_step",
            "prev_model_fn",
            "prev_model_hash",
            "prev_dataset_fn",
            "prev_dataset_hash",
            "prev_trainer_fn",
            "prev_trainer_hash",
            "collapsed_history",
            "data_transfer",
        ) * step_0.proj(
            transfer_step="0",
            prev_model_fn='""',
            prev_model_hash='""',
            prev_dataset_fn='""',
            prev_dataset_hash='""',
            prev_trainer_fn='""',
            prev_trainer_hash='""',
            collapsed_history='""',
            data_transfer="0",
        )  # train with "prev_"-entries empty

        if key_source is None:
            return base.proj()
        else:
            return key_source + base.proj()

    def get_full_config(self, key=None, include_state_dict=True, include_trainer=True):
        ret = super().get_full_config(
            key=key,
            include_state_dict=include_state_dict,
            include_trainer=include_trainer,
        )
        if key["transfer_step"] > 0:
            # retrieve previous key
            prev_prev_key = (
                self.CollapsedHistory & {"collapsed_history": key["collapsed_history"]}
            ).fetch1()
            prev_key = {
                "transfer_step": key["transfer_step"] - 1,
                "model_fn": key["prev_model_fn"],
                "model_hash": key["prev_model_hash"],
                "dataset_fn": key["prev_dataset_fn"],
                "dataset_hash": key["prev_dataset_hash"],
                "trainer_fn": key["prev_trainer_fn"],
                "trainer_hash": key["prev_trainer_hash"],
                "prev_model_fn": prev_prev_key["prev_model_fn"],
                "prev_model_hash": prev_prev_key["prev_model_hash"],
                "prev_dataset_fn": prev_prev_key["prev_dataset_fn"],
                "prev_dataset_hash": prev_prev_key["prev_dataset_hash"],
                "prev_trainer_fn": prev_prev_key["prev_trainer_fn"],
                "prev_trainer_hash": prev_prev_key["prev_trainer_hash"],
                "collapsed_history": prev_prev_key["prev_collapsed_history"],
                "seed": key["seed"],
            }

            # retrieve corresponding model state (and overwrite possibly retrieved state)
            if include_state_dict and (self.ModelStorage & prev_key):
                with tempfile.TemporaryDirectory() as temp_dir:
                    state_dict_path = (self.ModelStorage & prev_key).fetch1(
                        "model_state", download_path=temp_dir
                    )
                    ret["state_dict"] = torch.load(state_dict_path)
                    ret["model_config"]["transfer"] = True
            # retrieve data if present (if previous step did data transfer)
            if self.DataStorage & prev_key:
                with tempfile.TemporaryDirectory() as temp_dir:
                    data_path = (self.DataStorage & prev_key).fetch1(
                        "transfer_data", download_path=temp_dir
                    )
                    ret["dataset_config"]["transfer_data"] = np.load(data_path)
        return ret

    def make(self, key):
        """
        Given key specifying configuration for dataloaders, model and trainer,
        trains the model and saves the trained model.
        """

        # lookup the fabrikant corresponding to the current DJ user
        fabrikant_name = self.user_table.get_current_user()
        seed = (self.seed_table & key).fetch1("seed")

        # load everything
        dataloaders, model, trainer = self.load_model(
            key, include_trainer=True, include_state_dict=True, seed=seed
        )

        # define callback with pinging
        def call_back(**kwargs):
            self.connection.ping()
            self.call_back(**kwargs)

        if key["data_transfer"]:
            # generate data
            transfer_data, model_state = trainer(
                model=model, dataloaders=dataloaders, seed=seed, uid=key, cb=call_back
            )
            score = -1.0
            output = {}
        else:
            # model training
            score, output, model_state = trainer(
                model=model, dataloaders=dataloaders, seed=seed, uid=key, cb=call_back
            )

        with tempfile.TemporaryDirectory() as temp_dir:
            filename = make_hash(key)
            key["score"] = score
            key["output"] = output
            key["fabrikant_name"] = fabrikant_name
            comments = []
            comments.append((self.trainer_table & key).fetch1("trainer_comment"))
            comments.append((self.model_table & key).fetch1("model_comment"))
            comments.append((self.dataset_table & key).fetch1("dataset_comment"))
            key["comment"] = self.comment_delimitter.join(comments)

            key["current_model_fn"], key["current_model_hash"] = (
                self.model_table & key
            ).fetch1("model_fn", "model_hash")
            key["current_dataset_fn"], key["current_dataset_hash"] = (
                self.dataset_table & key
            ).fetch1("dataset_fn", "dataset_hash")
            key["current_trainer_fn"], key["current_trainer_hash"] = (
                self.trainer_table & key
            ).fetch1("trainer_fn", "trainer_hash")

            self.insert1(key)
            self.CollapsedHistory.add_entry(key)

            if key["data_transfer"]:
                data_path = os.path.join(temp_dir, filename + "_transfer_data.npz")
                np.savez(data_path, **transfer_data)
                key["transfer_data"] = data_path
                self.DataStorage.insert1(key, ignore_extra_fields=True)
            filename += ".pth.tar"
            filepath = os.path.join(temp_dir, filename)
            torch.save(model_state, filepath)
            key["model_state"] = filepath
            self.ModelStorage.insert1(key, ignore_extra_fields=True)
